---
permalink: config-linux/nvme-roce-setup-host-side-task.html 
sidebar: sidebar 
keywords: express linux configuration, software configuration, linux host, E2800, E5700, EF300, EF600, E-Series, eseries 
summary: 'La configuration de l"initiateur NVMe dans un environnement RoCE inclut l"installation et la configuration des packages rdma-core et nvme-cli, la configuration des adresses IP des initiateurs et la configuration de la couche NVMe-of sur l"hôte.' 
---
= Configurez l'initiateur NVMe sur RoCE sur l'hôte dans E-Series - Linux
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
La configuration de l'initiateur NVMe dans un environnement RoCE inclut l'installation et la configuration des packages rdma-core et nvme-cli, la configuration des adresses IP des initiateurs et la configuration de la couche NVMe-of sur l'hôte.

.Avant de commencer
Vous devez exécuter le tout dernier système d'exploitation compatible RHEL 7, RHEL 8, RHEL 9, SUSE Linux Enterprise Server 12 ou 15 Service Pack. Voir la https://mysupport.netapp.com/matrix["Matrice d'interopérabilité NetApp"^] pour obtenir la liste complète des dernières exigences.

.Étapes
. Installez les packages rdma et nvme-cli :
+
*SLES 12 ou SLES 15*

+
[listing]
----

# zypper install rdma-core
# zypper install nvme-cli
----
+
*RHEL 7, RHEL 8 ET RHEL 9*

+
[listing]
----

# yum install rdma-core
# yum install nvme-cli
----
. Pour RHEL 8 et RHEL 9, installez les scripts réseau :
+
*RHEL 8*

+
[listing]
----
# yum install network-scripts
----
+
*RHEL 9*

+
[listing]
----
# yum install NetworkManager-initscripts-updown
----
. Procurez-vous le NQN hôte, qui sera utilisé pour configurer l'hôte sur une matrice.
+
[listing]
----
# cat /etc/nvme/hostnqn
----
. Configurez les adresses IP IPv4 sur les ports ethernet utilisés pour connecter NVMe over RoCE. Pour chaque interface réseau, créez un script de configuration qui contient les différentes variables de cette interface.
+
Les variables utilisées à cette étape sont basées sur le matériel serveur et l'environnement réseau. Les variables incluent le `IPADDR` et `GATEWAY`. Voici des exemples d'instructions pour SLES et RHEL :

+
*SLES 12 et SLES 15*

+
Créez le fichier exemple `/etc/sysconfig/network/ifcfg-eth4` avec les contenus suivants.

+
[listing]
----
BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='192.168.1.87/24'
GATEWAY='192.168.1.1'
MTU=
NAME='MT27800 Family [ConnectX-5]'
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
Créez ensuite le fichier exemple `/etc/sysconfig/network/ifcfg-eth5`:

+
[listing]
----
BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='192.168.2.87/24'
GATEWAY='192.168.2.1'
MTU=
NAME='MT27800 Family [ConnectX-5]'
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
*RHEL 7 ou RHEL 8*

+
Créez le fichier exemple `/etc/sysconfig/network-scripts/ifcfg-eth4` avec les contenus suivants.

+
[listing]
----
BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='192.168.1.87/24’
GATEWAY='192.168.1.1'
MTU=
NAME='MT27800 Family [ConnectX-5]'
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
Créez ensuite le fichier exemple `/etc/sysconfig/network-scripts/ifcfg-eth5`:

+
[listing]
----
BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='192.168.2.87/24'
GATEWAY='192.168.2.1'
MTU=
NAME='MT27800 Family [ConnectX-5]'
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
*RHEL 9*

+
Utilisez le `nmtui` outil permettant d'activer et de modifier une connexion. Voici un exemple de fichier `/etc/NetworkManager/system-connections/eth4.nmconnection` l'outil génère :

+
[listing]
----

[connection]
id=eth4
uuid=<unique uuid>
type=ethernet
interface-name=eth4

[ethernet]
mtu=4200

[ipv4]
address1=192.168.1.87/24
method=manual

[ipv6]
addr-gen-mode=default
method=auto

[proxy]
----
+
Voici un exemple de fichier `/etc/NetworkManager/system-connections/eth5.nmconnection` l'outil génère :

+
[listing]
----

[connection]
id=eth5
uuid=<unique uuid>
type=ethernet
interface-name=eth5

[ethernet]
mtu=4200

[ipv4]
address1=192.168.2.87/24
method=manual

[ipv6]
addr-gen-mode=default
method=auto

[proxy]
----
. Activez les interfaces réseau :
+
[listing]
----

# ifup eth4
# ifup eth5
----
. Configurez la couche NVMe-of sur l'hôte. Créez le fichier suivant sous `/etc/modules-load.d/` pour charger le `nvme_rdma` module noyau et assurez-vous que le module noyau sera toujours activé, même après un redémarrage :
+
[listing]
----

# cat /etc/modules-load.d/nvme_rdma.conf
  nvme_rdma
----
. Redémarrez l'hôte.
+
Pour vérifier le `nvme_rdma` le module du noyau est chargé, exécutez la commande suivante :

+
[listing]
----
# lsmod | grep nvme
nvme_rdma              36864  0
nvme_fabrics           24576  1 nvme_rdma
nvme_core             114688  5 nvme_rdma,nvme_fabrics
rdma_cm               114688  7 rpcrdma,ib_srpt,ib_srp,nvme_rdma,ib_iser,ib_isert,rdma_ucm
ib_core               393216  15 rdma_cm,ib_ipoib,rpcrdma,ib_srpt,ib_srp,nvme_rdma,iw_cm,ib_iser,ib_umad,ib_isert,rdma_ucm,ib_uverbs,mlx5_ib,qedr,ib_cm
t10_pi                 16384  2 sd_mod,nvme_core
----

