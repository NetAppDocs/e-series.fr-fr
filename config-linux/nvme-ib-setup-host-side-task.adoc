---
permalink: config-linux/nvme-ib-setup-host-side-task.html 
sidebar: sidebar 
keywords: configure Linux host, express linux configuration, linux host, 
summary: 'La configuration d"un initiateur NVMe dans un environnement InfiniBand inclut l"installation et la configuration des packages infiniband, nvme-cli et rdma, la configuration des adresses IP des initiateurs et la configuration de la couche NVMe-of sur l"hôte.' 
---
= Configuration de la technologie NVMe over InfiniBand côté hôte
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
La configuration d'un initiateur NVMe dans un environnement InfiniBand inclut l'installation et la configuration des packages infiniband, nvme-cli et rdma, la configuration des adresses IP des initiateurs et la configuration de la couche NVMe-of sur l'hôte.

Vous devez exécuter la dernière version compatible du système d'exploitation RHEL 7, RHEL 8, RHEL9, SUSE Linux Enterprise Server 12 ou 15 Service Pack. Voir la https://mysupport.netapp.com/matrix["Matrice d'interopérabilité NetApp"^] pour obtenir la liste complète des dernières exigences.

.Étapes
. Installer les packages rdma, nvme-cli et infiniband :
+
*SLES 12 ou SLES 15*

+
[listing]
----

# zypper install infiniband-diags
# zypper install rdma-core
# zypper install nvme-cli
----
+
*RHEL 7, RHEL 8 OU RHEL 9*

+
[listing]
----

# yum install infiniband-diags
# yum install rdma-core
# yum install nvme-cli
----
. Activer `ipoib`. Modifiez le fichier /etc/rdma/rdma.conf et modifiez l'entrée à charger `ipoib`:
+
[listing]
----
IPOIB_LOAD=yes
----
. Procurez-vous le NQN hôte, qui sera utilisé pour configurer l'hôte sur une matrice.
+
[listing]
----
# cat /etc/nvme/hostnqn
----
. Vérifiez que les deux liaisons de port ib sont activées et que l'état = actif :
+
[listing]
----
# ibstat
----
+
[listing]
----
CA 'mlx4_0'
        CA type: MT4099
        Number of ports: 2
        Firmware version: 2.40.7000
        Hardware version: 1
        Node GUID: 0x0002c90300317850
        System image GUID: 0x0002c90300317853
        Port 1:
                State: Active
                Physical state: LinkUp
                Rate: 40
                Base lid: 4
                LMC: 0
                SM lid: 4
                Capability mask: 0x0259486a
                Port GUID: 0x0002c90300317851
                Link layer: InfiniBand
        Port 2:
                State: Active
                Physical state: LinkUp
                Rate: 56
                Base lid: 5
                LMC: 0
                SM lid: 4
                Capability mask: 0x0259486a
                Port GUID: 0x0002c90300317852
                Link layer: InfiniBand
----
. Configurez les adresses IP IPv4 sur les ports ib.
+
*SLES 12 ou SLES 15*

+
Créez le fichier /etc/sysconfig/network/ifcfg-ib0 avec le contenu suivant.

+
[listing]
----

BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='10.10.10.100/24'
IPOIB_MODE='connected'
MTU='65520'
NAME=
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
Créez ensuite le fichier /etc/sysconfig/network/ifcfg-ib1 :

+
[listing]
----

BOOTPROTO='static'
BROADCAST=
ETHTOOL_OPTIONS=
IPADDR='11.11.11.100/24'
IPOIB_MODE='connected'
MTU='65520'
NAME=
NETWORK=
REMOTE_IPADDR=
STARTMODE='auto'
----
+
*RHEL 7, RHEL 8 OU RHEL 9*

+
Créez le fichier /etc/sysconfig/network-scripts/ifcfg-ib0 avec le contenu suivant.

+
[listing]
----

CONNECTED_MODE=no
TYPE=InfiniBand
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
IPADDR='10.10.10.100/24'
DEFROUTE=no
IPV4=FAILURE_FATAL=yes
IPV6INIT=no
NAME=ib0
ONBOOT=yes
----
+
Créez ensuite le fichier /etc/sysconfig/network-scripts/ifcfg-ib1 :

+
[listing]
----

CONNECTED_MODE=no
TYPE=InfiniBand
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
IPADDR='11.11.11.100/24'
DEFROUTE=no
IPV4=FAILURE_FATAL=yes
IPV6INIT=no
NAME=ib1
ONBOOT=yes
----
. Activez le `ib` interface :
+
[listing]
----

# ifup ib0
# ifup ib1
----
. Vérifiez les adresses IP que vous allez utiliser pour vous connecter à la matrice. Exécutez cette commande pour les deux `ib0` et `ib1`:
+
[listing]
----

# ip addr show ib0
# ip addr show ib1
----
+
Comme indiqué dans l'exemple ci-dessous, l'adresse IP pour `ib0` est `10.10.10.255`.

+
[listing]
----
10: ib0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65520 qdisc pfifo_fast state UP group default qlen 256
    link/infiniband 80:00:02:08:fe:80:00:00:00:00:00:00:00:02:c9:03:00:31:78:51 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
    inet 10.10.10.255 brd 10.10.10.255 scope global ib0
       valid_lft forever preferred_lft forever
    inet6 fe80::202:c903:31:7851/64 scope link
       valid_lft forever preferred_lft forever
----
+
Comme indiqué dans l'exemple ci-dessous, l'adresse IP pour `ib1` est `11.11.11.255`.

+
[listing]
----
10: ib1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65520 qdisc pfifo_fast state UP group default qlen 256
    link/infiniband 80:00:02:08:fe:80:00:00:00:00:00:00:00:02:c9:03:00:31:78:51 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
    inet 11.11.11.255 brd 11.11.11.255 scope global ib0
       valid_lft forever preferred_lft forever
    inet6 fe80::202:c903:31:7851/64 scope link
       valid_lft forever preferred_lft forever
----
. Configurez la couche NVMe-of sur l'hôte. Créez les fichiers suivants sous /etc/modules-load.d/ pour charger le `nvme-rdma` module noyau et assurez-vous que le module noyau sera toujours activé, même après un redémarrage :
+
[listing]
----

# cat /etc/modules-load.d/nvme-rdma.conf
  nvme-rdma
----
+
Pour vérifier le `nvme-rdma` le module du noyau est chargé, exécutez la commande suivante :

+
[listing]
----

# lsmod | grep nvme
nvme_rdma              36864  0
nvme_fabrics           24576  1 nvme_rdma
nvme_core             114688  5 nvme_rdma,nvme_fabrics
rdma_cm               114688  7 rpcrdma,ib_srpt,ib_srp,nvme_rdma,ib_iser,ib_isert,rdma_ucm
ib_core               393216  15 rdma_cm,ib_ipoib,rpcrdma,ib_srpt,ib_srp,nvme_rdma,iw_cm,ib_iser,ib_umad,ib_isert,rdma_ucm,ib_uverbs,mlx5_ib,qedr,ib_cm
t10_pi                 16384  2 sd_mod,nvme_core
----

